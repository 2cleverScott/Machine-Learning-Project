{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Optimal Excercise with Machine Learning\"\nauthor: \"Scott Roberts\"\ndate: \"May 30, 2016\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\n\nlibrary(ggplot2)\nlibrary(caret)\nlibrary(parallel)\nlibrary(dplyr)\nlibrary(doMC)\nregisterDoMC(cores = 2)\n```\n\n##Synopsis\n\nThe goal of this project was to determine how an excercise was done by the study's participant. Building a model with the data provided from the accelerometers will involve classification models.  With the number of dimensions in the data a number of models could be chosen for this prediction problem. Of, the models built and analysed, the Random Forest model had the highest accuracy. \n\n##Analysis\n#Building the Model\n\nFirst, in order to build the model, we first load the training data and do some basic exploratory analysis and inspect the dimensions. There are a lot of NA values for over 100 of the variables.  So, we reduce the variables to include dimensions with useful data. the resulting dataframe had 51 variables.  53 variables had usefule data, but two variables,  were just totals for each window.\n\n\n\nGiven resulting the dataframe, I wanted to explore a few different models. To improve the models, I used Cross validation using 3 folds, as 10 folds pushed the limits of my available computing power. Waiting hours for a model to complete was not practical for this Practical Machine Learning project.  I thought about using other cross validation methods, but stuck with a basic k-fold cross validation. The first model would be Recursive Partitioning and Regression Trees, which resulted in a low accuracy which I found unsatisfactory. \n\nThe second model, the Generalized Boosted Regression Model was much more accurate than the CART model. There was still room for improvement, so I built a Random Forest model. This latest model proved to be quite accurate, slightly more accurate than the GBM model.\n\n##Conclusion\nFrom the resulting high Accuracy and Kappa, the Random Forest model performed much better, and used it for the 20 test cases.  Confident the model would predict all cases more accurately than the CART or gbm models.\n\n##Reference\n\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\n\nRead more: http://groupware.les.inf.puc-rio.br/har#ixzz4AxdwI56w\n\n##Appendix Code\n\n```{r echo=FALSE}\n\n### Obtain data from the site\nfurl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\ndownload.file(furl, destfile= \"pml-training.csv\", method=\"curl\")\npml.training <- read.csv(\"pml-training.csv\")\n\nfturl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\ndownload.file(fturl, destfile= \"pml-testing.csv\", method=\"curl\")\npml.testing <- read.csv(\"pml-testing.csv\")\n\npml1 <- subset(pml.training, select=c(classe,roll_belt,yaw_belt,gyros_belt_x,\n                                      gyros_belt_y,gyros_belt_z, accel_belt_x,accel_belt_y,accel_belt_z,magnet_belt_x,\n                                      magnet_belt_y,magnet_belt_z,pitch_belt, pitch_forearm,\n                                      roll_arm,pitch_arm,yaw_arm, roll_forearm, gyros_forearm_x,\n                                      gyros_forearm_y, gyros_forearm_z, accel_forearm_x, accel_forearm_y, \n                                      accel_forearm_z, magnet_forearm_x, magnet_forearm_y, magnet_forearm_z,total_accel_forearm,\n                                      roll_forearm, pitch_forearm, yaw_forearm,gyros_dumbbell_x, gyros_dumbbell_y,\n                                      gyros_dumbbell_z, accel_dumbbell_x, accel_dumbbell_y, accel_dumbbell_z,\n                                      magnet_dumbbell_x, magnet_dumbbell_y, magnet_dumbbell_z,\n                                      gyros_arm_x, gyros_arm_y, gyros_arm_z, accel_arm_x,accel_arm_y,\n                                      accel_arm_z, magnet_arm_x, magnet_arm_y, magnet_arm_z,roll_dumbbell,\n                                      pitch_dumbbell, yaw_dumbbell\n                                      \n))\nexcercisedf <- tbl_df(pml1)\n\n##Load Testing Data\n\n\npmltest <- subset(pml.testing,select=c(roll_belt,yaw_belt,gyros_belt_x,\n                                        gyros_belt_y,gyros_belt_z, accel_belt_x,accel_belt_y,accel_belt_z,magnet_belt_x,\n                                        magnet_belt_y,magnet_belt_z,pitch_belt, pitch_forearm,\n                                        roll_arm,pitch_arm,yaw_arm, roll_forearm, gyros_forearm_x,\n                                        gyros_forearm_y, gyros_forearm_z, accel_forearm_x, accel_forearm_y, \n                                        accel_forearm_z, magnet_forearm_x, magnet_forearm_y, magnet_forearm_z,total_accel_forearm,\n                                        roll_forearm, pitch_forearm, yaw_forearm,gyros_dumbbell_x, gyros_dumbbell_y,\n                                        gyros_dumbbell_z, accel_dumbbell_x, accel_dumbbell_y, accel_dumbbell_z,\n                                        magnet_dumbbell_x, magnet_dumbbell_y, magnet_dumbbell_z,\n                                        gyros_arm_x, gyros_arm_y, gyros_arm_z, accel_arm_x,accel_arm_y,\n                                        accel_arm_z, magnet_arm_x, magnet_arm_y, magnet_arm_z,roll_dumbbell,\n                                        pitch_dumbbell, yaw_dumbbell\n                                        \n))\n```\n#Explore the Data\n\n```{r}\n##Roll dumbell roll arm roll belt\nplot(excercisedf$classe, excercisedf$pitch_belt)\n\n```\n\n\n```{r}\nset.seed(93402)\n#Set the Cross Validation Folds\ncvcontrol <- trainControl(## 3-fold Cross-Validation\n    method = \"repeatedcv\",\n    number = 3,\n    repeats = 3)\n\n#Form models and choose the best, with the CART model, Gradient Boosted Machine and Random Forests\n\ncartfit  <- train(classe~., data=excercisedf,method=\"rpart\")\n\ncartfit\npredict(cartfit,pmltest)\nplot(cartfit)\n\n#Generalized Boosted Regression Model\nexcgbmfit <- train(classe~., data=excercisedf,method=\"gbm\", trControl=cvcontrol, verbose=FALSE )\nexcgbmfit\npredict(excgbmfit,pmltest)\nplot(excgbmfit)\n\n#Random Forest Model\nexcrffit <- train(classe~., data=excercisedf,method=\"rf\", trainControl=cvcontrol,verbose=FALSE )\nexcrffit\nplot(excrffit)\npredict(excrffit,pmltest)\n\n```\n",
    "created" : 1465448476770.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1020785731",
    "id" : "B7CDB880",
    "lastKnownWriteTime" : 1465448509,
    "last_content_update" : 1465448509889,
    "path" : "~/JHU/Machine Learning Project/PracticalMLProject.Rmd",
    "project_path" : "PracticalMLProject.Rmd",
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}